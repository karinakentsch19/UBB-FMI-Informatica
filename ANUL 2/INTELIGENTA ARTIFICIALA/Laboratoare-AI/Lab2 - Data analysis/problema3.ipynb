{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def readText(textFilePath):\n",
    "    \"\"\"\n",
    "    Read the entire text from a file\n",
    "    textFilePath: file that contains a text\n",
    "    return text\n",
    "    \"\"\"\n",
    "    stream = open(textFilePath)\n",
    "    return stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfSentencesInText(text):\n",
    "    \"\"\"\n",
    "    Returns the number of sentences in a text\n",
    "    text: text\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfWordsInText(text):\n",
    "    \"\"\"\n",
    "    Returns the number of words in a text\n",
    "    text: text\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfDistinctWordsInText(text):\n",
    "    \"\"\"\n",
    "    Returns the number of distinct words in a text\n",
    "    text: text\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    distinctWords = set(words)\n",
    "    return len(distinctWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestWordsInText(text):\n",
    "    \"\"\"\n",
    "    Returns the shortest words in a text\n",
    "    text: text\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    shortestWords = min(words, key=len)\n",
    "    return shortestWords\n",
    "\n",
    "\n",
    "def longestWordsInText(text):\n",
    "    \"\"\"\n",
    "    Returns the longest words in a text\n",
    "    text: text\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    longestWords = max(words, key=len)\n",
    "    return longestWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "def textWithoutDiacritics(text):\n",
    "    \"\"\"\n",
    "    Returns the text without diacritics\n",
    "    text: text\n",
    "    \"\"\"\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "from nltk.corpus import wordnet\n",
    "import rowordnet \n",
    "import spacy\n",
    "\n",
    "def removeSuffix(word):\n",
    "    \"\"\" \n",
    "    Removes the suffix of a word and returns that word\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"ro_core_news_sm\")\n",
    "    doc = nlp(word)\n",
    "    base_form = doc[0].lemma_\n",
    "    return base_form\n",
    "\n",
    "\n",
    "def getSynonyms(word):\n",
    "    \"\"\"\n",
    "    Returns the a words synonym\n",
    "    longestWord: string\n",
    "    \"\"\"\n",
    "    wn = rowordnet.RoWordNet()\n",
    "\n",
    "    word = removeSuffix(word)\n",
    "    synonyms = []\n",
    "    synsets_id = wn.synsets(literal= word) #id sinonime\n",
    "\n",
    "    for synset_id in synsets_id:\n",
    "        synset = wn(synset_id) #transforma indicele in cuvant\n",
    "        literals = list(synset.literals) #lista de sinonime pt cuvantul dat\n",
    "        for i in range(len(literals)): \n",
    "            for j in range(i+1, len(literals)):\n",
    "                synonyms.append((literals[i], literals[j]))\n",
    "    return synonyms\n",
    "\n",
    "def getSynonymOfLongestWord(text):\n",
    "    \"\"\" \n",
    "    Returns the longest word's sysnonyms\n",
    "    \"\"\"\n",
    "    max_word = longestWordsInText(text)\n",
    "    synonyms = getSynonyms(removeSuffix(max_word))\n",
    "    return synonyms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tests(text):\n",
    "    assert(numberOfSentencesInText(text) == 10)\n",
    "    assert(numberOfWordsInText(text) == 182)\n",
    "    assert(numberOfDistinctWordsInText(text) == 94)\n",
    "    assert(shortestWordsInText(text) == \"o\")\n",
    "    assert(longestWordsInText(text) == \"laboratoarele\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "182\n",
      "94\n",
      "o\n",
      "laboratoarele\n",
      "Mesaj de informare: \n",
      "Cursul si laboratoarele de Inteligenta Artificiala vor fi o \n",
      "provocare pentru toti. Suntem convinsi ca veti realiza proiecte \n",
      "foarte interesante. Va incurajam sa adresati intrebari atunci \n",
      "cand ceva nu e clar, atat in mod live, cat si folosind platforma \n",
      "Teams, canalul \"general\". \n",
      "Daca ati citit pana aici, va rugam sa lasati un mesaj pe canalul \n",
      "general cu textul \"Am citit textul pentru problema 3\". \n",
      "Mesaj de informare generat de ChatGPT:\n",
      "Stimati cursanti,\n",
      "Suntem incantati sa va avem in echipa noastra pentru Cursul si \n",
      "laboratoarele de Inteligenta Artificiala. Aceasta experienta va \n",
      "fi o adevarata provocare, dar suntem convinsi ca veti realiza \n",
      "proiecte extrem de interesante.\n",
      "Va incurajam sa fiti activi si sa adresati intrebari atunci cand \n",
      "ceva nu este clar. Fie ca este vorba de o discutie in timp real \n",
      "sau prin intermediul platformei Teams, canalul \"general\", suntem \n",
      "aici sa va sprijinim.\n",
      "Succes si sa inceapa aventura AI!\n",
      "Cu consideratie, Echipa de Inteligenta Artificiala\n",
      "                laborator == poligon_de_încercare\n",
      "                laborator == laborator_de_cercetare\n",
      "                laborator == laborator_de_științe\n",
      "   laborator_de_cercetare == laborator_de_științe\n",
      "                laborator == laborator_de_cercetare\n",
      "                laborator == laborator_de_științe\n",
      "   laborator_de_cercetare == laborator_de_științe\n",
      "                laborator == laborator_de_cercetare\n",
      "                laborator == laborator_de_științe\n",
      "   laborator_de_cercetare == laborator_de_științe\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #PROBLEMA 3\n",
    "    text = readText(\"texts.txt\")\n",
    "    tests(text)\n",
    "    #a\n",
    "    print(numberOfSentencesInText(text))\n",
    "\n",
    "    #b\n",
    "    print(numberOfWordsInText(text))\n",
    "\n",
    "    #c\n",
    "    print(numberOfDistinctWordsInText(text))\n",
    "\n",
    "    #d\n",
    "    print(shortestWordsInText(text))\n",
    "    print(longestWordsInText(text))\n",
    "\n",
    "    #e\n",
    "    print(textWithoutDiacritics(text))\n",
    "\n",
    "    #f\n",
    "    synonyms = getSynonymOfLongestWord(text)\n",
    "    for i in range(len(synonyms)):\n",
    "        print(\"{:>25} == {}\".format(synonyms[i][0], synonyms[i][1])) \n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
