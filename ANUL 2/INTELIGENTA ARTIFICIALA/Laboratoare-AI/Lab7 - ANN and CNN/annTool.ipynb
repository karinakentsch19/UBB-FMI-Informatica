{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificationPerformance(ground_truth, computed_values, positive_label):\n",
    "    \"\"\"\n",
    "    Returneaza TN (True Negative), FP(False Positive), FN(False Negative), TP(True Positive)\n",
    "    \"\"\"\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    \n",
    "    for i in range(0, len(ground_truth)):\n",
    "        #consideram malign = positive, benign = negative \n",
    "        if ground_truth[i] == positive_label:\n",
    "            if computed_values[i] == positive_label:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            if computed_values[i] != positive_label:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    return TN, FP, FN, TP\n",
    "\n",
    "\n",
    "def getAccuracy(TN, FP, FN, TP):\n",
    "    \"\"\" \n",
    "    accuracy represents the overall performance of classification model:\n",
    "    (TP+TN)/(TN+FP+FN+TP)\n",
    "    \"\"\"\n",
    "    if (TN+FP+FN+TP) == 0:\n",
    "        return 0\n",
    "    return (TP+TN)/(TN+FP+FN+TP)\n",
    "\n",
    "def getPrecision(FP, TP):\n",
    "    \"\"\"\n",
    "    precision indicates how accurate the positive predictions are \n",
    "    TP/(TP+FP)\n",
    "    \"\"\"\n",
    "    if (TP+FP) == 0:\n",
    "        return 0\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def getRecall(TP, FN):\n",
    "    \"\"\" \n",
    "    recall indicates the coverage of actual positive sample\n",
    "    TP/(TP+FN)\n",
    "    \"\"\"\n",
    "    if (TP+FN) == 0:\n",
    "        return 0\n",
    "    return TP/(TP+FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import neural_network\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "def readData() -> pd.DataFrame:\n",
    "    dataFrame = pd.read_csv(\"datas.csv\")\n",
    "    dataFrame = dataFrame.dropna()\n",
    "    return dataFrame\n",
    "\n",
    "\n",
    "def getTrainingAndValidationSets():\n",
    "    np.random.seed(5)\n",
    "    dataFrame = readData()\n",
    "    dataSize = dataFrame.shape[0]\n",
    "    \n",
    "    trainingIndexSet = np.random.choice(range(dataSize), size=int(0.7 * dataSize), replace=False)\n",
    "    validationIndexSet = [i for i in range(dataSize) if i not in trainingIndexSet]\n",
    "\n",
    "    trainingInputSet = [dataFrame[\"Photo\"].iloc[index] for index in trainingIndexSet]\n",
    "    trainingOutputSet = [dataFrame[\"Has Filter\"].iloc[index] for index in trainingIndexSet]\n",
    "\n",
    "    validationInputSet = [dataFrame[\"Photo\"].iloc[index] for index in validationIndexSet]\n",
    "    validationOutputSet = [dataFrame[\"Has Filter\"].iloc[index] for index in validationIndexSet]\n",
    "\n",
    "    return trainingInputSet, trainingOutputSet, validationInputSet, validationOutputSet\n",
    "\n",
    "\n",
    "def getRGBValuesForAllImages(inputImages, size):\n",
    "    rgbValues = []\n",
    "    for imagePath in inputImages:\n",
    "        rgbValues.append([])\n",
    "        image = Image.open(imagePath)\n",
    "        image = image.resize(size)\n",
    "\n",
    "        for pixel in list(image.getdata()):\n",
    "            r, g, b = pixel[0], pixel[1], pixel[2]\n",
    "            rgbValues[-1].append(r)\n",
    "            rgbValues[-1].append(g)\n",
    "            rgbValues[-1].append(b)\n",
    "    return rgbValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassifier(numberOfHiddenLayers, activationFunction, trainingInputs, trainingOutputs):\n",
    "    classifier = neural_network.MLPClassifier(hidden_layer_sizes=(numberOfHiddenLayers,), activation=activationFunction, max_iter=100, solver='sgd', verbose=10, random_state=1, learning_rate_init=.1)\n",
    "    classifier.fit(trainingInputs, trainingOutputs)\n",
    "    return classifier\n",
    "\n",
    "def testClassifier(numberOfHiddenLayers, activationFunction, size):\n",
    "    trainingInputSet, trainingOutputSet, validationInputSet, validationOutputSet = getTrainingAndValidationSets()\n",
    "    trainingInputs = getRGBValuesForAllImages(trainingInputSet, size)\n",
    "    trainingOutputs = trainingOutputSet\n",
    "    classifier = getClassifier(numberOfHiddenLayers, activationFunction, trainingInputs, trainingOutputs)\n",
    "\n",
    "    validaitonInputs = getRGBValuesForAllImages(validationInputSet, size)\n",
    "    outputs = classifier.predict(validaitonInputs)\n",
    "\n",
    "    TN, FP, FN, TP = clasificationPerformance(validationOutputSet, outputs, \"YES\")\n",
    "    accuracy = getAccuracy(TN, FP, FN, TP)\n",
    "    precision = getPrecision(FP, TP)\n",
    "    recall = getRecall(TP, FN)\n",
    "    print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\".format(accuracy, precision, recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testarea clasificatorului - influenta (hyper)parametrilor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70778898\n",
      "Iteration 2, loss = 0.74905773\n",
      "Iteration 3, loss = 0.73851225\n",
      "Iteration 4, loss = 0.71609072\n",
      "Iteration 5, loss = 0.70337733\n",
      "Iteration 6, loss = 0.68988153\n",
      "Iteration 7, loss = 0.67822156\n",
      "Iteration 8, loss = 0.66700212\n",
      "Iteration 9, loss = 0.65875862\n",
      "Iteration 10, loss = 0.64818881\n",
      "Iteration 11, loss = 0.63938426\n",
      "Iteration 12, loss = 0.63259722\n",
      "Iteration 13, loss = 0.62680634\n",
      "Iteration 14, loss = 0.62183166\n",
      "Iteration 15, loss = 0.61754836\n",
      "Iteration 16, loss = 0.61383912\n",
      "Iteration 17, loss = 0.61061695\n",
      "Iteration 18, loss = 0.60779903\n",
      "Iteration 19, loss = 0.60531744\n",
      "Iteration 20, loss = 0.60311425\n",
      "Iteration 21, loss = 0.60114260\n",
      "Iteration 22, loss = 0.59936483\n",
      "Iteration 23, loss = 0.59775120\n",
      "Iteration 24, loss = 0.59627839\n",
      "Iteration 25, loss = 0.59492801\n",
      "Iteration 26, loss = 0.59368547\n",
      "Iteration 27, loss = 0.59253887\n",
      "Iteration 28, loss = 0.59147829\n",
      "Iteration 29, loss = 0.59049516\n",
      "Iteration 30, loss = 0.58958187\n",
      "Iteration 31, loss = 0.58873147\n",
      "Iteration 32, loss = 0.58793758\n",
      "Iteration 33, loss = 0.58719426\n",
      "Iteration 34, loss = 0.58649599\n",
      "Iteration 35, loss = 0.58583772\n",
      "Iteration 36, loss = 0.58521483\n",
      "Iteration 37, loss = 0.58462319\n",
      "Iteration 38, loss = 0.58405913\n",
      "Iteration 39, loss = 0.58351949\n",
      "Iteration 40, loss = 0.58300156\n",
      "Iteration 41, loss = 0.58250308\n",
      "Iteration 42, loss = 0.58202218\n",
      "Iteration 43, loss = 0.58155735\n",
      "Iteration 44, loss = 0.58110739\n",
      "Iteration 45, loss = 0.58067135\n",
      "Iteration 46, loss = 0.58024850\n",
      "Iteration 47, loss = 0.57983824\n",
      "Iteration 48, loss = 0.57944011\n",
      "Iteration 49, loss = 0.57905372\n",
      "Iteration 50, loss = 0.57867874\n",
      "Iteration 51, loss = 0.57831485\n",
      "Iteration 52, loss = 0.57796176\n",
      "Iteration 53, loss = 0.57761915\n",
      "Iteration 54, loss = 0.57728670\n",
      "Iteration 55, loss = 0.57696409\n",
      "Iteration 56, loss = 0.57665097\n",
      "Iteration 57, loss = 0.57634698\n",
      "Iteration 58, loss = 0.57605174\n",
      "Iteration 59, loss = 0.57576488\n",
      "Iteration 60, loss = 0.57548603\n",
      "Iteration 61, loss = 0.57521482\n",
      "Iteration 62, loss = 0.57495088\n",
      "Iteration 63, loss = 0.57469386\n",
      "Iteration 64, loss = 0.57444342\n",
      "Iteration 65, loss = 0.57419924\n",
      "Iteration 66, loss = 0.57396102\n",
      "Iteration 67, loss = 0.57372848\n",
      "Iteration 68, loss = 0.57350134\n",
      "Iteration 69, loss = 0.57327937\n",
      "Iteration 70, loss = 0.57306233\n",
      "Iteration 71, loss = 0.57285000\n",
      "Iteration 72, loss = 0.57264220\n",
      "Iteration 73, loss = 0.57243873\n",
      "Iteration 74, loss = 0.57223942\n",
      "Iteration 75, loss = 0.57204411\n",
      "Iteration 76, loss = 0.57185265\n",
      "Iteration 77, loss = 0.57166490\n",
      "Iteration 78, loss = 0.57148072\n",
      "Iteration 79, loss = 0.57129998\n",
      "Iteration 80, loss = 0.57112257\n",
      "Iteration 81, loss = 0.57094836\n",
      "Iteration 82, loss = 0.57077725\n",
      "Iteration 83, loss = 0.57060913\n",
      "Iteration 84, loss = 0.57044390\n",
      "Iteration 85, loss = 0.57028146\n",
      "Iteration 86, loss = 0.57012173\n",
      "Iteration 87, loss = 0.56996462\n",
      "Iteration 88, loss = 0.56981003\n",
      "Iteration 89, loss = 0.56965790\n",
      "Iteration 90, loss = 0.56950814\n",
      "Iteration 91, loss = 0.56936068\n",
      "Iteration 92, loss = 0.56921545\n",
      "Iteration 93, loss = 0.56907239\n",
      "Iteration 94, loss = 0.56893143\n",
      "Iteration 95, loss = 0.56879250\n",
      "Iteration 96, loss = 0.56865556\n",
      "Iteration 97, loss = 0.56852055\n",
      "Iteration 98, loss = 0.56838740\n",
      "Iteration 99, loss = 0.56825608\n",
      "Iteration 100, loss = 0.56812654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37037037037037035\n",
      "Precision: 0.625\n",
      "Recall: 0.2631578947368421\n"
     ]
    }
   ],
   "source": [
    "testClassifier(numberOfHiddenLayers=50, activationFunction='tanh', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.89642108\n",
      "Iteration 2, loss = 0.63412296\n",
      "Iteration 3, loss = 0.69794357\n",
      "Iteration 4, loss = 0.70072646\n",
      "Iteration 5, loss = 0.63723151\n",
      "Iteration 6, loss = 0.62925079\n",
      "Iteration 7, loss = 0.67682228\n",
      "Iteration 8, loss = 0.65885101\n",
      "Iteration 9, loss = 0.62245201\n",
      "Iteration 10, loss = 0.61761372\n",
      "Iteration 11, loss = 0.61410803\n",
      "Iteration 12, loss = 0.61147288\n",
      "Iteration 13, loss = 0.68786381\n",
      "Iteration 14, loss = 0.68688523\n",
      "Iteration 15, loss = 0.65172629\n",
      "Iteration 16, loss = 0.65254372\n",
      "Iteration 17, loss = 0.65369573\n",
      "Iteration 18, loss = 0.65473163\n",
      "Iteration 19, loss = 0.65558411\n",
      "Iteration 20, loss = 0.65622271\n",
      "Iteration 21, loss = 0.65663998\n",
      "Iteration 22, loss = 0.65684512\n",
      "Iteration 23, loss = 0.65685938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.7037037037037037\n",
      "Precision: 0.375\n",
      "Recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "testClassifier(numberOfHiddenLayers=50, activationFunction='tanh', size=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79744221\n",
      "Iteration 2, loss = 0.89811683\n",
      "Iteration 3, loss = 0.85280716\n",
      "Iteration 4, loss = 0.70312468\n",
      "Iteration 5, loss = 0.68155767\n",
      "Iteration 6, loss = 0.71984763\n",
      "Iteration 7, loss = 0.74669320\n",
      "Iteration 8, loss = 0.74502487\n",
      "Iteration 9, loss = 0.72723440\n",
      "Iteration 10, loss = 0.71239141\n",
      "Iteration 11, loss = 0.71017996\n",
      "Iteration 12, loss = 0.71709960\n",
      "Iteration 13, loss = 0.72494467\n",
      "Iteration 14, loss = 0.72939316\n",
      "Iteration 15, loss = 0.73113195\n",
      "Iteration 16, loss = 0.73253373\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.6296296296296297\n",
      "Precision: 0.625\n",
      "Recall: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "testClassifier(numberOfHiddenLayers=10, activationFunction='tanh', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79384520\n",
      "Iteration 2, loss = 0.78165137\n",
      "Iteration 3, loss = 1.99374320\n",
      "Iteration 4, loss = 5.60516322\n",
      "Iteration 5, loss = 1.36340960\n",
      "Iteration 6, loss = 6.52872083\n",
      "Iteration 7, loss = 0.66827362\n",
      "Iteration 8, loss = 2.08375171\n",
      "Iteration 9, loss = 5.86981249\n",
      "Iteration 10, loss = 2.03327417\n",
      "Iteration 11, loss = 5.38224083\n",
      "Iteration 12, loss = 1.00655038\n",
      "Iteration 13, loss = 2.88263334\n",
      "Iteration 14, loss = 4.09815890\n",
      "Iteration 15, loss = 0.59241448\n",
      "Iteration 16, loss = 0.99419568\n",
      "Iteration 17, loss = 1.75542240\n",
      "Iteration 18, loss = 2.42638052\n",
      "Iteration 19, loss = 2.69942793\n",
      "Iteration 20, loss = 1.16065202\n",
      "Iteration 21, loss = 1.12925286\n",
      "Iteration 22, loss = 1.20153064\n",
      "Iteration 23, loss = 1.13703687\n",
      "Iteration 24, loss = 0.99728844\n",
      "Iteration 25, loss = 0.96798196\n",
      "Iteration 26, loss = 0.80548307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.5555555555555556\n",
      "Precision: 0.25\n",
      "Recall: 0.25\n"
     ]
    }
   ],
   "source": [
    "testClassifier(numberOfHiddenLayers=200, activationFunction='tanh', size=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70254298\n",
      "Iteration 2, loss = 0.70038306\n",
      "Iteration 3, loss = 0.68606223\n",
      "Iteration 4, loss = 0.68804875\n",
      "Iteration 5, loss = 0.68464230\n",
      "Iteration 6, loss = 0.68567518\n",
      "Iteration 7, loss = 0.68464784\n",
      "Iteration 8, loss = 0.68438763\n",
      "Iteration 9, loss = 0.68440728\n",
      "Iteration 10, loss = 0.68440633\n",
      "Iteration 11, loss = 0.68441041\n",
      "Iteration 12, loss = 0.68441382\n",
      "Iteration 13, loss = 0.68441458\n",
      "Iteration 14, loss = 0.68441286\n",
      "Iteration 15, loss = 0.68440860\n",
      "Iteration 16, loss = 0.68440187\n",
      "Iteration 17, loss = 0.68439275\n",
      "Iteration 18, loss = 0.68438138\n",
      "Iteration 19, loss = 0.68436788\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.7037037037037037\n",
      "Precision: 0.0\n",
      "Recall: 0\n"
     ]
    }
   ],
   "source": [
    "testClassifier(numberOfHiddenLayers=50, activationFunction='logistic', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14.31189336\n",
      "Iteration 2, loss = 17.93621003\n",
      "Iteration 3, loss = 5026343.77331306\n",
      "Iteration 4, loss = 4954687308.09134960\n",
      "Iteration 5, loss = 10075449157.00680351\n",
      "Iteration 6, loss = 16222098596.91878319\n",
      "Iteration 7, loss = 22999828687.46001816\n",
      "Iteration 8, loss = 30108833965.59874725\n",
      "Iteration 9, loss = 37324261489.73435211\n",
      "Iteration 10, loss = 44480171015.42481232\n",
      "Iteration 11, loss = 51456722602.41344452\n",
      "Iteration 12, loss = 58169960458.33673859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.7037037037037037\n",
      "Precision: 0.0\n",
      "Recall: 0\n"
     ]
    }
   ],
   "source": [
    "testClassifier(numberOfHiddenLayers=100, activationFunction='relu', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 16.06751443\n",
      "Iteration 2, loss = 17.26917865\n",
      "Iteration 3, loss = 118964.79247998\n",
      "Iteration 4, loss = 20956157.24016824\n",
      "Iteration 5, loss = 42531836.98216401\n",
      "Iteration 6, loss = 68418387.16202605\n",
      "Iteration 7, loss = 96955696.05411910\n",
      "Iteration 8, loss = 126883175.02684046\n",
      "Iteration 9, loss = 157255439.50506768\n",
      "Iteration 10, loss = 187374851.41160014\n",
      "Iteration 11, loss = 216737635.34009638\n",
      "Iteration 12, loss = 244990913.78694135\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.7037037037037037\n",
      "Precision: 0.0\n",
      "Recall: 0\n"
     ]
    }
   ],
   "source": [
    "testClassifier(numberOfHiddenLayers=10, activationFunction='relu', size=(128, 128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
